{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCbrvpeBQQcD",
        "outputId": "6a300d33-a308-4466-c5a4-e9077a8679d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Using subset of the data (10,000 samples)...\n",
            "Applying SMOTE...\n",
            "Training Random Forest...\n",
            "Random Forest Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99      1720\n",
            "           1       0.90      1.00      0.95       280\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.95      0.99      0.97      2000\n",
            "weighted avg       0.99      0.98      0.98      2000\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1720\n",
            "           1       0.95      0.97      0.96       280\n",
            "\n",
            "    accuracy                           0.99      2000\n",
            "   macro avg       0.97      0.98      0.98      2000\n",
            "weighted avg       0.99      0.99      0.99      2000\n",
            "\n",
            "Training SVM...\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99      1720\n",
            "           1       0.90      1.00      0.95       280\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.95      0.99      0.97      2000\n",
            "weighted avg       0.99      0.98      0.98      2000\n",
            "\n",
            "Training LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "LSTM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99      1720\n",
            "           1       0.87      1.00      0.93       280\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.93      0.99      0.96      2000\n",
            "weighted avg       0.98      0.98      0.98      2000\n",
            "\n",
            "Training CNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "CNN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99      1720\n",
            "           1       0.89      0.99      0.94       280\n",
            "\n",
            "    accuracy                           0.98      2000\n",
            "   macro avg       0.94      0.99      0.96      2000\n",
            "weighted avg       0.98      0.98      0.98      2000\n",
            "\n",
            "Training Autoencoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Autoencoder Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.90      1720\n",
            "           1       0.03      0.01      0.02       280\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.44      0.48      0.46      2000\n",
            "weighted avg       0.74      0.81      0.77      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ------------------------------- Load and Prepare Dataset -------------------------------\n",
        "print(\"Loading dataset...\")\n",
        "\n",
        "# List of file paths to be merged\n",
        "file_paths = ['UNSW-NB15_1.csv', 'UNSW-NB15_2.csv', 'UNSW-NB15_3.csv', 'UNSW-NB15_4.csv']\n",
        "# Read and merge all CSV files into a single DataFrame\n",
        "df = pd.concat([pd.read_csv(file, low_memory=False) for file in file_paths], ignore_index=True)\n",
        "\n",
        "# Encode the label column\n",
        "label_encoder = LabelEncoder()\n",
        "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "# Drop non-numeric columns and fill NaNs\n",
        "df = df.select_dtypes(include=[np.number]).fillna(0)\n",
        "\n",
        "# Optionally reduce dataset size for faster testing\n",
        "print(\"Using subset of the data (10,000 samples)...\")\n",
        "df = df.sample(n=10000, random_state=42)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE for class imbalance\n",
        "print(\"Applying SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Early stopping for deep learning models\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# ------------------------------- Random Forest -------------------------------\n",
        "print(\"Training Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)  # Reduced depth\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# ------------------------------- XGBoost -------------------------------\n",
        "print(\"Training XGBoost...\")\n",
        "xgboost = XGBClassifier(random_state=42)\n",
        "xgboost.fit(X_train_res, y_train_res)\n",
        "y_pred_xgb = xgboost.predict(X_test)\n",
        "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# ------------------------------- SVM -------------------------------\n",
        "print(\"Training SVM...\")\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train_res, y_train_res)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# ------------------------------- LSTM -------------------------------\n",
        "print(\"Training LSTM...\")\n",
        "X_train_lstm = X_train_res.reshape((X_train_res.shape[0], X_train_res.shape[1], 1))\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(20, activation='relu', input_shape=(X_train_lstm.shape[1], 1)),  # Reduced units\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_lstm, y_train_res, epochs=3, batch_size=32,\n",
        "               validation_data=(X_test_lstm, y_test), callbacks=[early_stop], verbose=0)\n",
        "\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = (y_pred_lstm > 0.5)\n",
        "print(\"LSTM Classification Report:\\n\", classification_report(y_test, y_pred_lstm))\n",
        "\n",
        "# ------------------------------- CNN -------------------------------\n",
        "print(\"Training CNN...\")\n",
        "X_train_cnn = X_train_res.reshape((X_train_res.shape[0], X_train_res.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),  # Reduced filters\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train_cnn, y_train_res, epochs=3, batch_size=32,\n",
        "              validation_data=(X_test_cnn, y_test), callbacks=[early_stop], verbose=0)\n",
        "\n",
        "y_pred_cnn = cnn_model.predict(X_test_cnn)\n",
        "y_pred_cnn = (y_pred_cnn > 0.5)\n",
        "print(\"CNN Classification Report:\\n\", classification_report(y_test, y_pred_cnn))\n",
        "\n",
        "# ------------------------------- Autoencoder -------------------------------\n",
        "print(\"Training Autoencoder...\")\n",
        "input_dim = X_train_res.shape[1]\n",
        "autoencoder = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(input_dim,)),  # Reduced size\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(input_dim, activation='sigmoid')\n",
        "])\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "autoencoder.fit(X_train_res, X_train_res, epochs=3, batch_size=32,\n",
        "                validation_data=(X_test, X_test), callbacks=[early_stop], verbose=0)\n",
        "\n",
        "X_test_pred = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)\n",
        "threshold = np.percentile(mse, 95)\n",
        "y_pred_autoencoder = (mse > threshold).astype(int)\n",
        "\n",
        "print(\"Autoencoder Classification Report:\\n\", classification_report(y_test, y_pred_autoencoder))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}